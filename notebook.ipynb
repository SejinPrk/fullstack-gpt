{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42cea1eb-e73e-4d38-b10d-2e908d70e082",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from sympy.physics.units import temperature\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# from langchain.llms.openai import OpenAI\n",
    "# from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     temperature = 0.1,\n",
    "#     # streaming=True,\n",
    "#     # callbacks=[StreamingStdOutCallbackHandler()]\n",
    "# )\n",
    "\n",
    "# chat = OpenAI(\n",
    "#     temperature = 0.1,\n",
    "#     max_tokens=450,\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "# )\n",
    "\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "# chat = load_llm(\"model.json\")\n",
    "\n",
    "# chat\n",
    "\n",
    "# t= PromptTemplate(\n",
    "#     template=\"What is the capital of {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# 위와 동일한 내용\n",
    "# t = PromptTemplate(template=\"What is the capital of {country}?\")\n",
    "\n",
    "# t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd41dec7-3bcc-48c2-988f-a63728dd3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a vegetarian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "#     (\"human\", \"{recipe}\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca73f7cf-5272-4111-993d-a866591b71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "# final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "\n",
    "# final_chain.invoke({\n",
    "#     \"cuisine\": \"indian\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf16bcb1-ea97-408a-8f8b-85eae74d5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"question\": \"What do you know about France?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         Here is what I know:\n",
    "#         Capital: Paris\n",
    "#         Language: French\n",
    "#         Food: Wine and Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Italy?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Rome\n",
    "#         Language: Italian\n",
    "#         Food: Pizza and Pasta\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Greece?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Athens\n",
    "#         Language: Greek\n",
    "#         Food: Souvlaki and Feta Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# chat.predict(\"What do you know about France?\")\n",
    "\n",
    "# example_template = \"\"\"\n",
    "#     Human: {question}\n",
    "#     AI: {answer}\n",
    "# \"\"\"\n",
    "\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "#     suffix=\"Human: What do you know about {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "# chain = prompt | chat\n",
    "\n",
    "# chain.invoke({\"country\": \"Germany\"})\n",
    "# chain.invoke({\"country\": \"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "093b3e4a-c2c7-424d-a8d5-ce8f839894e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"country\": \"France\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         Here is what I know:\n",
    "#         Capital: Paris\n",
    "#         Language: French\n",
    "#         Food: Wine and Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"country\": \"Italy\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Rome\n",
    "#         Language: Italian\n",
    "#         Food: Pizza and Pasta\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"country\": \"Greece\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Athens\n",
    "#         Language: Greek\n",
    "#         Food: Souvlaki and Feta Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "#     suffix=\"Human: What do you know about {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# chain = prompt | chat\n",
    "\n",
    "# chain.invoke({\"country\": \"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f089c7e-8935-4afe-a23e-7fd892ddd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "# from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"question\": \"What do you know about France?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         Here is what I know:\n",
    "#         Capital: Paris\n",
    "#         Language: French\n",
    "#         Food: Wine and Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Italy?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Rome\n",
    "#         Language: Italian\n",
    "#         Food: Pizza and Pasta\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Greece?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Athens\n",
    "#         Language: Greek\n",
    "#         Food: Souvlaki and Feta Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# class RandomExampleSelector(BaseExampleSelector):\n",
    "#     def __init__(self, examples):\n",
    "#         self.examples = examples\n",
    "\n",
    "#     def add_example(self, example):\n",
    "#         self.examples.append(example)\n",
    "\n",
    "#     def select_examples(self, input_variables):\n",
    "#         from random import choice\n",
    "\n",
    "#         return [choice(self.examples)]\n",
    "        \n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=80\n",
    "# )\n",
    "\n",
    "# example_selector = RandomExampleSelector(\n",
    "#     examples=examples,\n",
    "# )\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     example_selector=example_selector,\n",
    "#     suffix=\"Human: What do you know about {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51792350-b0fe-4a86-899c-08bd9069b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt=load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "# prompt.format(country=\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "203cd41a-1462-48c7-b51d-9263cb34bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "# intro = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     You are a role playing assistant. And you are impersonating a {character}.\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# example = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     This is an example of how you talk:\n",
    "\n",
    "#     Human: {example_question}\n",
    "#     You: {example_answer}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# start = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     {intro}\n",
    "\n",
    "#     {example}\n",
    "\n",
    "#     Human: {question}\n",
    "#     You:\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# final = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     {intro}\n",
    "\n",
    "#     {example}\n",
    "\n",
    "#     {start}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# prompts = [\n",
    "#     (\"intro\", intro),\n",
    "#     (\"example\", example),\n",
    "#     (\"start\", start),\n",
    "# ]\n",
    "\n",
    "# full_prompt = PipelinePromptTemplate(\n",
    "#     final_prompt=final, \n",
    "#     pipeline_prompts=prompts\n",
    "# )\n",
    "\n",
    "# # full_prompt.format(\n",
    "# #     character=\"Pirate\",\n",
    "# #     example_question=\"What is your location?\",\n",
    "# #     example_answer=\"Arrg! That is a secret! Arg arg!!\",\n",
    "# #     question=\"What is ur fav food?\"\n",
    "# # )\n",
    "\n",
    "# chain = full_prompt | chat\n",
    "\n",
    "# chain.invoke({\n",
    "#     \"character\":\"Pirate\",\n",
    "#     \"example_question\":\"What is your location?\",\n",
    "#     \"example_answer\":\"Arrg! That is a secret! Arg arg!!\",\n",
    "#     \"question\":\"What is ur fav food?\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f8d4244-6710-4953-ab79-5f79241514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_llm_cache, set_debug\n",
    "# from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache()) # cache data\n",
    "# set_llm_cache(SQLiteCache(\"cache.db\")) # set cache and save it to database\n",
    "# set_debug(True) # show logs\n",
    "\n",
    "# chat.predict(\"How do you make Italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa1bbb7d-3433-4684-869f-488b636947dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it answers immediately\n",
    "# chat.predict(\"How do you make Italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1df506bd-0dd4-4c7e-880a-2bbe88fb0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# with get_openai_callback() as usage: \n",
    "#     a = chat.predict(\"What is the recipe for soju\")\n",
    "#     b = chat.predict(\"What is the recipe for bread\")\n",
    "#     print(a, b, \"\\n\")\n",
    "#     # print(usage)\n",
    "#     # print(usage.total_cost)\n",
    "#     # print(usage.total_tfrom langchain.llms.loading import load_llm\n",
    "# okens) # completion_tokens 도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc77c888-fa66-4e1e-8fd3-33f343cf36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory\n",
    "\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory = ConversationBufferMemory() # String 형식으로 출력\n",
    "# memory = ConversationBufferMemory(return_messages=True) # chat model 형식으로 출력\n",
    "\n",
    "# memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "\n",
    "# memory.load_memory_variables({}) # 비효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d1fbd86-3746-4725-902a-238ad4b74ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최근 n개 메세지만 저장 -> the most recent conversation\n",
    "# from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# memory = ConversationBufferWindowMemory(\n",
    "#     return_messages=True,\n",
    "#     k= 4, # 몇 개를 저장할지 설정\n",
    "# )\n",
    "\n",
    "# def add_message(input, output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# add_message(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9639c85b-06c8-4fcb-86be-c13806d27685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(2,2)\n",
    "# add_message(3,3)\n",
    "# add_message(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a630f5f-8bff-4fe7-9727-0e663cbaa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b760ff5-9c52-4d33-80c1-acef09b902ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23c7f2a6-0f80-4bb3-9795-7190b9632b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({}) # (1,1) 이 사라진 것을 확인 가능 -> 이전 대화를 기억할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e320eb3-bb92-4c02-8201-da1bf1dd463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary -> useful for a very long conversation\n",
    "# 글자수가 많기 때문에 초반에는 더 많은 메모리와 토큰을 차지함\n",
    "# 그러나 대화가 진행될수록 저장된 메세지가 매우 많아지면서 모두 연결됨\n",
    "# 요약이 토큰 양을 줄일 수 있음 -> 더 효율적\n",
    "\n",
    "# from langchain.memory import ConversationSummaryMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# def add_message(input,output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# def get_history():\n",
    "#     return memory.load_memory_variables({})\n",
    "\n",
    "# add_message(\"Hi, I'm Camille. I live in South Korea.\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a49d0e2-257b-488a-8928-c6c1c1cfc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"South Korea is so pretty.\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a33a261-9f00-4891-8f06-a13666e8d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4964a1a1-aa02-4c7c-af97-1612c653162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 최근 메세지, 가장 오래된 메세지 모두 요약\n",
    "# window buffer memory + buffer window memory\n",
    "\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=150, # 최대 토큰 수\n",
    "#     return_messages=True, # chat model\n",
    "# )\n",
    "\n",
    "# def add_message(input,output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# def get_history():\n",
    "#     return memory.load_memory_variables({})\n",
    "\n",
    "# add_message(\"Hi, I'm Camille. I live in South Korea.\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93194d8f-c903-4db1-8830-f648ab74f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b084735f-c406-4de5-a905-8e35e2fd4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"South Korea is so pretty.\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18c09746-866b-4ce3-a9bc-6961cf649402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "409fbd8e-12f8-4d9f-b242-1b4245e2c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03644869-113f-4ebc-b473-55de7db3b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f6b64b4-1a38-44b0-a101-e9a711d0dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a70b027e-18ee-49dd-9b2c-224bf4762393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2032896b-2649-4cc8-9c92-77334feeb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is Colombia from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dcb57f1-95bf-488e-ac27-c0e79fef57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d1c508f-521b-4b26-a946-94cce1a25ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is China from Peru?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "258c9003-6e1b-42f1-bb5c-520ee083a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20fb8852-584d-4adf-a6c7-24b1a7166b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationKGMemory (Knowledge Graph)\n",
    "# 가장 중요한 것들만 뽑아내는 요약본 -> 대화에서 entity 추출\n",
    "\n",
    "# from langchain.memory import ConversationKGMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationKGMemory(\n",
    "#     llm=llm,\n",
    "#     return_messages=True, # chat model\n",
    "# )\n",
    "\n",
    "# def add_message(input,output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# add_message(\"Hi, I'm Camille. I live in South Korea.\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71990c5a-3e0a-485e-b3ab-f11f2b564211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({\"input\":\"who is Camille\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39439124-a281-4a88-bae0-08dcb90ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"Camille likes Kimchi\", \"Sounds delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f907f8b3-ca49-4bc4-8f2d-08b4adfa897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({\"input\":\"what does Camille like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a23ee3a0-b195-4a11-937c-4895c4b9c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory on LLMChain\n",
    "# interaction의 토큰 수가 limit(80)보다 많으면 가장 오래된 interaction을 요약해줌\n",
    "\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=80,\n",
    "#     # verbose=True, # log\n",
    "# )\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=PromptTemplate.from_template(\"{question}\")\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Camille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37465a01-d319-477a-891e-d69973c804c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ac7fe55-e0e3-473c-8b35-62a6b68bf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"What's my name\") # it doesn't work -> history isn't added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2bbe02d0-e033-4472-9826-b70208fd29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({}) # Update는 진행하지만 프롬프트에 그 히스토리를 반영하지 않는다 -> template 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46671d1d-9053-4238-b2fa-7b2c36f27a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 메모리 객체 생성 (memory_key 설정 포함)\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"chat_history\" # 템플릿에서 동일한 이름 사용해야 함\n",
    "# )\n",
    "\n",
    "# template = \"\"\"\n",
    "#     You are a helpful AI talking to a human. \n",
    "\n",
    "#     {chat_history}\n",
    "#     Human:{question}\n",
    "#     You:\n",
    "# \"\"\"\n",
    "\n",
    "# # 템플릿 적용\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=PromptTemplate.from_template(template)\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Camille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88569369-da69-445f-8046-2f0243ba2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"I live in Seoul\") # prompt가 history를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35fd7753-ef78-4926-82d8-5ead92a4b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"What's my name\") # it remembers my name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb1d640a-5311-4bcc-8135-4ac552ffb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({}) # 텍스트 형식으로 출력됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63108b4b-4c73-42ee-8dd7-6d43ab437809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat based memory: 대화 기반의 형식으로 받고 싶다면 \n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=True\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful AI talking to human\"),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"), # 우리는 메세지가 얼마나 많고 누구로부터 왔는지 모름. 위의 memory class로 대체될 것.\n",
    "#     (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=prompt,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Camille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "16b13f0c-d2c2-4997-b952-3dbf82c5ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "503aae30-7b4b-4ebd-b276-7f51e1b0aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"What's my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e4ea2c7-82ae-4d22-8ae6-2cb4ff1ae3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 커스터마이징 메모리: LCEL based Memory\n",
    "\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema.runnable import RunnablePassthrough # Prompt가 format 되기 전에 함수 실행시키는 걸 허락해준다\n",
    "# from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# # memory와 prompt는 변하지 않는다.\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"history\",\n",
    "#     return_messages=True\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful AI talking to human\"),\n",
    "#     MessagesPlaceholder(variable_name=\"history\"), # 우리는 메세지가 얼마나 많고 누구로부터 왔는지 모름. 위의 memory class로 대체될 것.\n",
    "#     (\"human\", \"{question}\")\n",
    "# ]) \n",
    "\n",
    "# def load_memory(_): # _: ignore\n",
    "#     # print(input)\n",
    "#     return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "# # 체인을 호출하는 함수 생성 -> 수동으로 메모리 관리하는 것보다 편리한 방법\n",
    "# def invoke_chain(question): \n",
    "#     result = chain.invoke({\"question\": question})\n",
    "#     memory.save_context(\n",
    "#         {\"input\":question}, \n",
    "#         {\"output\":result.content}\n",
    "#     )\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82fb7199-b05f-4b51-b0a7-ce75930aab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke_chain(\"My name is Camille\") #  대화 내용이 메모리에 저장되고, LLM은 이전 대화 내용을 기억하면서 응답함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d81f752e-077d-4334-b3b8-6d9549a8ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke_chain(\"what is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b8eaced5-d9fc-49aa-a0dd-af0c30c3cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "52d87956-3719-4128-b2b9-e80ac5b774d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "# loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a782d7f-a51e-4aa7-993d-09a0686f7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# # loader.load()\n",
    "# len(loader.load()) # 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cdfe127d-5949-48ec-a58d-90dcdd1671f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# splitter.split_documents(docs)\n",
    "\n",
    "# len(loader.load_and_split(text_splitter=splitter)) # 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecc21628-c475-4241-9e8f-50d9921c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\",\n",
    "#     chunk_size=600,\n",
    "#     chunk_overlap=100\n",
    "# )\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# splitter.split_documents(docs)\n",
    "\n",
    "# len(loader.load_and_split(text_splitter=splitter)) # 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1afd438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     separator=\"\\n\",\n",
    "#     chunk_size=600,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len, # counting texts (different from human's counting method )\n",
    "# )\n",
    "# # tokens != characters\n",
    "# # tiktoken은 tokens 수를 세는 것 -> from_tiktoken_encoder을 사용함으로써 사람과 동일한 방법으로 텍스트 카운팅이 가능\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b08ca005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vector store\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embedder= OpenAIEmbeddings()\n",
    "\n",
    "# # vector = embedder.embed_query(\"Hi\")\n",
    "# # len(vector) # how many dimensions? 1536\n",
    "\n",
    "# vector = embedder.embed_documents([\n",
    "#   \"hi\",\n",
    "#   \"how\",\n",
    "#   \"are\",\n",
    "#   \"you + longer sentenes\"\n",
    "# ])\n",
    "\n",
    "# # vector\n",
    "# # len(vector) # 4\n",
    "\n",
    "# print(len(vector), len(vector[0])) # 4개 벡터, 1536개의 차원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4628142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.storage import LocalFileStore\n",
    "\n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "\n",
    "# vectorstore = Chroma.from_documents(docs, cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ff730dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = vectorstore.similarity_search(\"where does winston live?\")\n",
    "\n",
    "# len(results) # 4\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b76b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "\n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "\n",
    "# vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#   llm=llm,\n",
    "#   chain_type=\"map_rerank\", # map_reduce, stuff, refine, map_rerank\n",
    "#   retriever=vectorstore.as_retriever(),\n",
    "# )\n",
    "\n",
    "# chain.run(\"where does winston live?\") # 1st floor 'Winston Smith lives in Victory Mansions.'\n",
    "# chain.run(\"Describe Victory Mansions\") # 2nd floor 'Victory Mansions is a place where Winston lives. It is a place that is not very nice.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ca2ff56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#   temperature=0.1,\n",
    "# )\n",
    "\n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "\n",
    "# vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# retriever = vectorstore.as_retriever() \n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#   (\"system\", \"You are a helpful assistant. Answer the questions using only the following context. If you don't know the answer, just say you don't know, don't make it up:\\n{context}\"),\n",
    "#   (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# chain = (\n",
    "#   {\n",
    "#     \"context\": retriever, \n",
    "#     \"question\": RunnablePassthrough(), \n",
    "#     # \"extra\": RunnablePassthrough(),\n",
    "#   } \n",
    "#   | prompt \n",
    "#   | llm\n",
    "# )\n",
    "\n",
    "# chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9178240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# \n",
    "# llm = ChatOpenAI(\n",
    "#   temperature=0.1,\n",
    "# )\n",
    "# \n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "# \n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "# \n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "# \n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# \n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "# \n",
    "# vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "# \n",
    "# retriever = vectorstore.as_retriever() \n",
    "# \n",
    "# # list of docs\n",
    "# \n",
    "# # for doc in list of docs | prompt | llm \n",
    "# \n",
    "# # for response in list of llms response | put them all together \n",
    "# \n",
    "# # final doc | prompt | llm\n",
    "# \n",
    "# map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "#   [\n",
    "#     (\n",
    "#       \"system\",\n",
    "#       \"\"\"\n",
    "#       Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "#       Return any relevant text verbatim.\n",
    "#       -----\n",
    "#       {context}\n",
    "#       \"\"\",\n",
    "#     ),\n",
    "#     (\"human\", \"{question}\"),\n",
    "#   ]\n",
    "# )\n",
    "# \n",
    "# map_doc_chain =  map_doc_prompt | llm \n",
    "# \n",
    "# def map_docs(inputs):\n",
    "#   # print(inputs)\n",
    "#   documents = inputs[\"documents\"]\n",
    "#   question = inputs[\"question\"]\n",
    "#   # results = []\n",
    "#   # for document in documents:\n",
    "#   #   result = map_doc_chain.invoke({\n",
    "#   #     \"context\": document.page_content,\n",
    "#   #     \"question\": question,\n",
    "#   #   }).content\n",
    "#   #   results.append(result)\n",
    "#   # results = \"\\n\\n\".join(results)\n",
    "#   # return results\n",
    "# \n",
    "#   return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "#     \"context\": doc.page_content,\n",
    "#     \"question\": question,\n",
    "#     }).content\n",
    "#     for doc in documents\n",
    "#   ) # list of docs\n",
    "# \n",
    "# \n",
    "# map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    "# RunnableLambda(map_docs)\n",
    "# \n",
    "# # {\n",
    "# #   \"documents\": [Documents],\n",
    "# #   \"question\": \"Describe Victory Mansions\"\n",
    "# # }\n",
    "# \n",
    "# final_prompt = ChatPromptTemplate.from_messages([\n",
    "#   (\n",
    "#     \"system\", \n",
    "#     \"\"\"\n",
    "#     Given the following extracted parts of a long document and a question, \n",
    "#     create a final answer. \n",
    "#     If you don't know the answer, just say you don't know, don't make it up.\n",
    "#     -----\n",
    "#     {context}\n",
    "#     \"\"\",\n",
    "#   ),\n",
    "#   (\"human\", \"{question}\"),\n",
    "# ])\n",
    "# \n",
    "# chain = {\"context\": map_chain,\"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "# \n",
    "# chain.invoke(\"Where does Winston go to work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1104dba4-8e84-42a9-af29-c598477c9903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe62e0b781d0337b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     model_kwargs={\n",
    "#         \"max_new_tokens\": 250,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# chain = prompt | llm\n",
    "\n",
    "# chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3c03daf5-9d43-4d4a-9358-208c1cf1b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms.gpt4all import GPT4All\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# \n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"You are a helpful assistant that defines words. Define this word: {word}.\"\n",
    "# )\n",
    "# \n",
    "# llm = GPT4All(\n",
    "#     model=\"./falcon.bin\",\n",
    "# )\n",
    "# \n",
    "# chain = prompt | llm\n",
    "# \n",
    "# chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c10f8facdd057bcd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# function = {\n",
    "#     \"name\": \"create_quiz\",\n",
    "#     \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "#     \"parameters\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"questions\": {\n",
    "#                 \"type\": \"array\",\n",
    "#                 \"items\": {\n",
    "#                     \"type\": \"object\",\n",
    "#                     \"properties\": {\n",
    "#                         \"question\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                         },\n",
    "#                         \"answers\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\n",
    "#                                 \"type\": \"object\",\n",
    "#                                 \"properties\": {\n",
    "#                                     \"answer\": {\n",
    "#                                         \"type\": \"string\",\n",
    "#                                     },\n",
    "#                                     \"correct\": {\n",
    "#                                         \"type\": \"boolean\",\n",
    "#                                     },\n",
    "#                                 },\n",
    "#                                 \"required\": [\"answer\", \"correct\"],\n",
    "#                             },\n",
    "#                         },\n",
    "#                     },\n",
    "#                     \"required\": [\"question\", \"answers\"],\n",
    "#                 },\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"questions\"],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0.1,\n",
    "# ).bind(\n",
    "#     function_call={\n",
    "#         \"name\": \"create_quiz\",\n",
    "#     },\n",
    "#     functions=[\n",
    "#         function,\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "# chain = prompt | llm\n",
    "\n",
    "# response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "# response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "# response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6039749a-15cd-41ff-a867-bac6b9e4f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# for question in json.loads(response)[\"questions\"]:\n",
    "#     print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878d5bc",
   "metadata": {},
   "source": [
    "# Investor GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61ea7637",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from typing import Type\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.agents import initialize_agent, AgentType\n",
    "# from langchain.tools import StructuredTool, Tool, BaseTool\n",
    "# from pydantic import BaseModel, Field\n",
    "# from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0.1,\n",
    "#     model_name=\"gpt-4o-mini\",\n",
    "# )\n",
    "\n",
    "# alpha_vantage_api_key = os.environ.get(\"ALPHAVANTAGE_API_KEY\")\n",
    "\n",
    "# class CompanyOverviewArgsSchema(BaseModel):\n",
    "#     symbol: str = Field(description=\"The stock symbol of the company. Example: AAPL for Apple Inc.\")\n",
    "\n",
    "# class CompanyOverviewTool(BaseTool):\n",
    "#     name: str = \"CompanyOverview\"\n",
    "#     description: str = \"\"\"\n",
    "#     Use this to get an overview of the financials of the company.\n",
    "#     You should enter a stock symbol.\n",
    "#     \"\"\"\n",
    "#     args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "#     def _run(self, symbol):\n",
    "#         r = requests.get(\n",
    "#             f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "#         )\n",
    "#         return r.json()\n",
    "\n",
    "# class CompanyIncomeStatementTool(BaseTool):\n",
    "#     name: str = \"CompanyIncomeStatement\"\n",
    "#     description: str = \"\"\"\n",
    "#     Use this to get the income statement of the company.\n",
    "#     You should enter a stock symbol.\n",
    "#     \"\"\"\n",
    "#     args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "#     def _run(self, symbol):\n",
    "#         r = requests.get(\n",
    "#             f\"https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "#         )\n",
    "#         return r.json()[\"annualReports\"]\n",
    "\n",
    "# class CompanyStockPerformanceTool(BaseTool):\n",
    "#     name: str = \"CompanyStockPerformanceStatement\"\n",
    "#     description: str = \"\"\"\n",
    "#     Use this to get the weekly performance of the company stock.\n",
    "#     You should enter a stock symbol.\n",
    "#     \"\"\"\n",
    "#     args_schema: Type[CompanyOverviewArgsSchema] = CompanyOverviewArgsSchema\n",
    "\n",
    "#     def _run(self, symbol):\n",
    "#         r = requests.get(\n",
    "#             f\"https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={symbol}&apikey={alpha_vantage_api_key}\"\n",
    "#         )\n",
    "#         return r.json()\n",
    "\n",
    "\n",
    "# class StockMarketSymbolSearchToolArgsSchema(BaseModel):\n",
    "#     query: str = Field(description=\"The query you will search for\")\n",
    "\n",
    "# class StockMarketSymbolSearchTool(BaseTool):\n",
    "#     name: str = \"StockMarketSymbolSearchTool\"\n",
    "#     description: str = \"\"\"\n",
    "#     Use this to search for stock market symbols for a company.\n",
    "#     It takes a query as an argument.\n",
    "#     Example query: Stock market symbol for Apple Company \n",
    "#     \"\"\"\n",
    "#     args_schema: Type[StockMarketSymbolSearchToolArgsSchema] = StockMarketSymbolSearchToolArgsSchema\n",
    "\n",
    "#     def _run(self, query):\n",
    "#         ddg = DuckDuckGoSearchAPIWrapper()\n",
    "#         return ddg.run(query)\n",
    "    \n",
    "# agent = initialize_agent(\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "#     agent=AgentType.OPENAI_FUNCTIONS,\n",
    "#     handle_parsing_errors = True,\n",
    "#     tools=[ \n",
    "#         StockMarketSymbolSearchTool(),\n",
    "#         CompanyOverviewTool(),\n",
    "#         CompanyIncomeStatementTool(),\n",
    "#         CompanyStockPerformanceTool(),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# prompt = \"Give me information on Cloudflare's stock, considering its financials, income statements, and stock performance. Help me analyze if it's a potential good investment. Also tell me what symbol does the stock have. \"\n",
    "\n",
    "# agent.invoke(prompt)\n",
    "    \n",
    "# # Use StructuredTool\n",
    "# def plus(a, b):\n",
    "#     return a + b\n",
    "\n",
    "# agent = initialize_agent(\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "#     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     tools=[\n",
    "#         StructuredTool.from_function(\n",
    "#             func=plus,\n",
    "#             name=\"Sum Calculator\",\n",
    "#             description=\"Use this to perform sums of two numbers. This tool takes two arguments, both should be numbers.\",\n",
    "#         ),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Use Zero-Shot-React\n",
    "# def plus(inputs):\n",
    "#     a, b = inputs.split(\",\")\n",
    "#     return float(a) + float(b)\n",
    "\n",
    "# agent = initialize_agent(\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     handle_parsing_errors = True,\n",
    "#     tools=[\n",
    "#         Tool.from_function(\n",
    "#             func=plus,\n",
    "#             name=\"Sum Calculator\",\n",
    "#             description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample: 1,2\",\n",
    "#         ),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Use BaseTool\n",
    "# class CalculatorToolArgsSchema(BaseModel):\n",
    "#     a: float = Field(description=\"The first number\")\n",
    "#     b: float = Field(description=\"The second number\")\n",
    "# \n",
    "# class CalculatorTool(BaseTool):\n",
    "#     name: str = \"CalculatorTool\"\n",
    "#     description: str = \"\"\"\n",
    "#     Use this to perform sums of two numbers.\n",
    "#     The first and second arguments should be numbers.\n",
    "#     Only receives two arguments.\n",
    "#     \"\"\"\n",
    "#     args_schema: Type[BaseModel] = CalculatorToolArgsSchema\n",
    "# \n",
    "#     def _run(self, a, b):\n",
    "#         return a + b\n",
    "\n",
    "\n",
    "# prompt = \"Cost of $2354.64 + $9245.4 + $66.2 + $0.99 + $5325.3029\"\n",
    "\n",
    "# llm.invoke(prompt) # LLM은 자체적으로 정확한 계산을 할 수 없음 -> agent를 사용\n",
    "\n",
    "# agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e5f12b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import create_sql_agent, AgentType\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "# from langchain.sql_database import SQLDatabase\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0.1, \n",
    "#     model_name=\"gpt-4o-mini\",\n",
    "# )\n",
    "# db = SQLDatabase.from_uri(\"sqlite:///movies.sqlite\")\n",
    "# toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "# agent = create_sql_agent(\n",
    "#   llm=llm,\n",
    "#   toolkit=toolkit,\n",
    "#   agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#   verbose=True,\n",
    "# )\n",
    "\n",
    "# # agent.invoke(\"Give me the 5 directors that have the highest grossing films.\")\n",
    "# agent.invoke(\"Give me the movies that have the highest votes but the lowest budgets and give me the name of their directors. Also include their gross revenue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170edde9",
   "metadata": {},
   "source": [
    "# Meeting GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bcb5d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess \n",
    "# from pydub import AudioSegment\n",
    "# import math\n",
    "# \n",
    "# def extract_audio_from_video(video_path, audio_path):\n",
    "#   command = [\n",
    "#     \"ffmpeg\", \n",
    "#     \"-i\", \n",
    "#     video_path, \n",
    "#     \"-vn\", \n",
    "#     audio_path\n",
    "#   ]\n",
    "#   subprocess.run(command)\n",
    "# \n",
    "# def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):\n",
    "#   track = AudioSegment.from_mp3(audio_path)\n",
    "#   chunk_len = chunk_size * 60 * 1000\n",
    "#   chunks = math.ceil(len(track) / chunk_len)\n",
    "#   for i in range(chunks):\n",
    "#     start_time = i * chunk_len\n",
    "#     end_time = (i + 1) * chunk_len\n",
    "# \n",
    "#     chunk = track[start_time:end_time]\n",
    "# \n",
    "#     chunk.export(f\"{chunks_folder}/chunk_{i}.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9d2fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_five = track[:five_minutes] # 5분짜리 오디오 파일 생성\n",
    "\n",
    "# first_five.export(\"./files/first_five.mp3\", format=\"mp3\") # mp3로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b1e804e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut_audio_in_chunks(\"./files/podcast.mp3\", 10, \"./files/chunks\") # 10분짜리 오디오 파일 생성  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1c60db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# import glob\n",
    "# \n",
    "# client = OpenAI()\n",
    "# \n",
    "# # print(transcript.text)\n",
    "# \n",
    "# def transcribe_chunks(chunk_folder, destination):\n",
    "#     files = glob.glob(f\"{chunk_folder}/*.mp3\")\n",
    "#     final_transcript = \"\"\n",
    "#     for file in files:\n",
    "#         with open(file, \"rb\") as audio_file:\n",
    "#             transcript = client.audio.transcriptions.create(\n",
    "#                 model=\"whisper-1\",\n",
    "#                 file=audio_file\n",
    "#             )\n",
    "#         final_transcript += transcript.text\n",
    "#     with open(destination, \"a\") as file:\n",
    "#         file.write(final_transcript)\n",
    "# \n",
    "# transcribe_chunks(\"./files/chunks\", \"./files/transcript.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4783f579aa8a7cbd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# import openai  # ✅ 올바른 import\n",
    "# from pinecone import Pinecone\n",
    "\n",
    "# load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# def get_embedding(text: str):\n",
    "#     response = openai.Embedding.create(\n",
    "#         model=\"text-embedding-ada-002\",\n",
    "#         input=text\n",
    "#     )\n",
    "#     return response['data'][0]['embedding']\n",
    "\n",
    "# pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# # 인덱스 가져오기\n",
    "# index = pc.Index(\"recipes\")\n",
    "\n",
    "# # 예시 텍스트 임베딩 및 업로드\n",
    "# texts = [\n",
    "#     \"How to cook tofu: Cut, fry, season.\",\n",
    "#     \"Tofu is a great plant-based protein.\",\n",
    "#     \"Tofu can be used in stews, stir-fry, and soups.\"\n",
    "# ]\n",
    "\n",
    "# vectors = [\n",
    "#     {\n",
    "#         \"id\": f\"vec-{i}\",\n",
    "#         \"values\": get_embedding(text),\n",
    "#         \"metadata\": {\"text\": text}\n",
    "#     }\n",
    "#     for i, text in enumerate(texts)\n",
    "# ]\n",
    "\n",
    "# # Pinecone에 벡터 업로드\n",
    "# index.upsert(vectors=vectors)\n",
    "\n",
    "# # 검색 쿼리 임베딩 및 유사도 검색\n",
    "# query_text = \"What are some tofu dishes?\"\n",
    "# query_vec = get_embedding(query_text)\n",
    "\n",
    "# res = index.query(vector=query_vec, top_k=3, include_metadata=True)\n",
    "\n",
    "# # 결과 출력\n",
    "# for match in res['matches']:\n",
    "#     print(f\"[score: {match['score']:.3f}] {match['metadata']['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11113d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Tax Effect Of Unusual Items\": 0.0, \"Tax Rate For Calcs\": 0.241, \"Normalized EBITDA\": 134661000000.0, \"Net Income From Continuing Operation Net Minority Interest\": 93736000000.0, \"Reconciled Depreciation\": 11445000000.0, \"Reconciled Cost Of Revenue\": 210352000000.0, \"EBITDA\": 134661000000.0, \"EBIT\": 123216000000.0, \"Net Interest Income\": NaN, \"Interest Expense\": NaN, \"Interest Income\": NaN, \"Normalized Income\": 93736000000.0, \"Net Income From Continuing And Discontinued Operation\": 93736000000.0, \"Total Expenses\": 267819000000.0, \"Total Operating Income As Reported\": 123216000000.0, \"Diluted Average Shares\": 15408095000.0, \"Basic Average Shares\": 15343783000.0, \"Diluted EPS\": 6.08, \"Basic EPS\": 6.11, \"Diluted NI Availto Com Stockholders\": 93736000000.0, \"Net Income Common Stockholders\": 93736000000.0, \"Net Income\": 93736000000.0, \"Net Income Including Noncontrolling Interests\": 93736000000.0, \"Net Income Continuous Operations\": 93736000000.0, \"Tax Provision\": 29749000000.0, \"Pretax Income\": 123485000000.0, \"Other Income Expense\": 269000000.0, \"Other Non Operating Income Expenses\": 269000000.0, \"Net Non Operating Interest Income Expense\": NaN, \"Interest Expense Non Operating\": NaN, \"Interest Income Non Operating\": NaN, \"Operating Income\": 123216000000.0, \"Operating Expense\": 57467000000.0, \"Research And Development\": 31370000000.0, \"Selling General And Administration\": 26097000000.0, \"Gross Profit\": 180683000000.0, \"Cost Of Revenue\": 210352000000.0, \"Total Revenue\": 391035000000.0, \"Operating Revenue\": 391035000000.0}'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "# import yfinance\n",
    "# import json\n",
    "\n",
    "\n",
    "# def get_ticker(inputs):\n",
    "#     ddg = DuckDuckGoSearchAPIWrapper()\n",
    "#     company_name = inputs[\"company_name\"]\n",
    "#     return ddg.run(f\"Ticker symbol of {company_name}\")\n",
    "\n",
    "# def get_income_statement(inputs):\n",
    "#     ticker = inputs[\"ticker\"]\n",
    "#     stock = yfinance.Ticker(ticker)\n",
    "#     income_df = stock.income_stmt\n",
    "#     latest_period = income_df.columns[0]\n",
    "#     latest_data = income_df[latest_period]\n",
    "#     return json.dumps(latest_data.to_dict())\n",
    "\n",
    "# def get_balance_sheet(inputs):\n",
    "#     ticker = inputs[\"ticker\"]\n",
    "#     stock = yfinance.Ticker(ticker)\n",
    "#     return json.dumps(stock.balance_sheet.to_json())\n",
    "\n",
    "# def get_daily_stock_performance(inputs):\n",
    "#     ticker = inputs[\"ticker\"]\n",
    "#     stock = yfinance.Ticker(ticker)\n",
    "#     return json.dumps(stock.history(period=\"3mo\").to_json())\n",
    "\n",
    "# functions = [\n",
    "#     {\n",
    "#         \"name\": \"get_ticker\", \n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": \"get_ticker\",\n",
    "#             \"description\": \"Given the name of a company returns its ticker symbol\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"company_name\": {\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"description\": \"The name of the company\",\n",
    "#                     }\n",
    "#                 },\n",
    "#                 \"required\": [\"company_name\"],\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"get_income_statement\",\n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": \"get_income_statement\",\n",
    "#             \"description\": \"Given a ticker symbol (i.e AAPL) returns the company's income statement.\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"ticker\": {\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"description\": \"Ticker symbol of the company\",\n",
    "#                     },\n",
    "#                 },\n",
    "#                 \"required\": [\"ticker\"],\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"get_balance_sheet\",\n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": \"get_balance_sheet\",\n",
    "#             \"description\": \"Given a ticker symbol (i.e AAPL) returns the company's balance sheet.\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"ticker\": {\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"description\": \"Ticker symbol of the company\",\n",
    "#                     },\n",
    "#                 },\n",
    "#                 \"required\": [\"ticker\"],\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"get_daily_stock_performance\", \n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": \"get_daily_stock_performance\",\n",
    "#             \"description\": \"Given a ticker symbol (i.e AAPL) returns the performance of the stock for the last 100 days.\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"ticker\": {\n",
    "\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"description\": \"Ticker symbol of the company\",\n",
    "#                     },\n",
    "#                 },\n",
    "#                 \"required\": [\"ticker\"],\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# get_income_statement({\"ticker\": \"AAPL\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load the .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# assistant = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-4\", \n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You help users do research on publicly traded companies.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"What is the income statement of Apple?\"} \n",
    "#     ],\n",
    "#     functions=functions,\n",
    "#     function_call=\"auto\" \n",
    "# )\n",
    "\n",
    "# assistant_id = \"asst_hxxtN2JboIASLO0lrclhmrMe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758c8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-BUqQpoMocxKTuH139Mu8Pj0f8hBUl at 0x2b91890a2a0> JSON: {\n",
       "  \"id\": \"chatcmpl-BUqQpoMocxKTuH139Mu8Pj0f8hBUl\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1746691087,\n",
       "  \"model\": \"gpt-4-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"As an AI developed by OpenAI, I cannot provide personal investment advice or opinions. However, I can provide some steps you could follow to make an informed decision:\\n\\n1. **Understand the Business**: Salesforce is a cloud computing service as a software (SaaS) company that provides various software solutions and a platform for users and developers to develop and distribute custom software.\\n\\n2. **Financial Health check**: You can use tools like 'functions.get_income_statement' and 'functions.get_balance_sheet' to fetch the financial data and understand the profitability, cashflow, and debts of Salesforce.\\n\\n3. **Stock's Performance**: The 'functions.get_daily_stock_performance' function can provide you information about the stock's daily performance which will help you understand how the stock has been performing recently.\\n\\n4. **Market Sentiment**: Market sentiment is the overall attitude of investors toward a particular security or larger financial market. It helps in predicting the future behavior of financial markets.\\n\\n5. **Study Analysts' Recommendations**: A number of brokerages have issued ratings and price targets for Salesforce. You can follow their recommendations to guide your decisions.\\n\\nRemember, investing in the stock market is always subject to risks and it's important to do your own exhaustive research before making any investment decisions.\",\n",
       "        \"refusal\": null,\n",
       "        \"annotations\": []\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 72,\n",
       "    \"completion_tokens\": 254,\n",
       "    \"total_tokens\": 326,\n",
       "    \"prompt_tokens_details\": {\n",
       "      \"cached_tokens\": 0,\n",
       "      \"audio_tokens\": 0\n",
       "    },\n",
       "    \"completion_tokens_details\": {\n",
       "      \"reasoning_tokens\": 0,\n",
       "      \"audio_tokens\": 0,\n",
       "      \"accepted_prediction_tokens\": 0,\n",
       "      \"rejected_prediction_tokens\": 0\n",
       "    }\n",
       "  },\n",
       "  \"service_tier\": \"default\",\n",
       "  \"system_fingerprint\": null\n",
       "}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thread = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-4\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"I want to know if the Salesforce stock is a good buy\",\n",
    "#         }\n",
    "#     ],\n",
    "#     functions=functions,\n",
    "#     function_call=\"auto\" \n",
    "# )\n",
    "# thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bd7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Tax Effect Of Unusual Items\": 0.0, \"Tax Rate For Calcs\": 0.21, \"Normalized EBITDA\": 62047000.0, \"Total Unusual Items\": 0.0, \"Total Unusual Items Excluding Goodwill\": 0.0, \"Net Income From Continuing Operation Net Minority Interest\": -78800000.0, \"Reconciled Depreciation\": 127722000.0, \"Reconciled Cost Of Revenue\": 378702000.0, \"EBITDA\": 62047000.0, \"EBIT\": -65675000.0, \"Net Interest Income\": 82230000.0, \"Interest Expense\": 5196000.0, \"Interest Income\": 87426000.0, \"Normalized Income\": -78800000.0, \"Net Income From Continuing And Discontinued Operation\": -78800000.0, \"Total Expenses\": 1824387000.0, \"Total Operating Income As Reported\": -154761000.0, \"Diluted Average Shares\": 341411000.0, \"Basic Average Shares\": 341411000.0, \"Diluted EPS\": -0.23, \"Basic EPS\": -0.23, \"Diluted NI Availto Com Stockholders\": -78800000.0, \"Net Income Common Stockholders\": -78800000.0, \"Net Income\": -78800000.0, \"Net Income Including Noncontrolling Interests\": -78800000.0, \"Net Income Continuous Operations\": -78800000.0, \"Tax Provision\": 7929000.0, \"Pretax Income\": -70871000.0, \"Other Income Expense\": 1660000.0, \"Other Non Operating Income Expenses\": 1660000.0, \"Special Income Charges\": 0.0, \"Other Special Charges\": NaN, \"Net Non Operating Interest Income Expense\": 82230000.0, \"Interest Expense Non Operating\": 5196000.0, \"Interest Income Non Operating\": 87426000.0, \"Operating Income\": -154761000.0, \"Operating Expense\": 1445685000.0, \"Research And Development\": 421374000.0, \"Selling General And Administration\": 1024311000.0, \"Selling And Marketing Expense\": 745791000.0, \"General And Administrative Expense\": 278520000.0, \"Other Gand A\": 278520000.0, \"Gross Profit\": 1290924000.0, \"Cost Of Revenue\": 378702000.0, \"Total Revenue\": 1669626000.0, \"Operating Revenue\": 1669626000.0}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_income_statement({\"ticker\": \"NET\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdce05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I am currently unable to directly view or manage files that users upload. However, I can certainly answer questions about file uploading processes, common issues, or suggest solutions based on the information you provide me.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"You help users with their questions on the files they upload.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
