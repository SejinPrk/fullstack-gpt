{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42cea1eb-e73e-4d38-b10d-2e908d70e082",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from sympy.physics.units import temperature\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# from langchain.llms.openai import OpenAI\n",
    "# from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     temperature = 0.1,\n",
    "#     # streaming=True,\n",
    "#     # callbacks=[StreamingStdOutCallbackHandler()]\n",
    "# )\n",
    "\n",
    "# chat = OpenAI(\n",
    "#     temperature = 0.1,\n",
    "#     max_tokens=450,\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "# )\n",
    "\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "# chat = load_llm(\"model.json\")\n",
    "\n",
    "# chat\n",
    "\n",
    "# t= PromptTemplate(\n",
    "#     template=\"What is the capital of {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# 위와 동일한 내용\n",
    "# t = PromptTemplate(template=\"What is the capital of {country}?\")\n",
    "\n",
    "# t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd41dec7-3bcc-48c2-988f-a63728dd3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a vegetarian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "#     (\"human\", \"{recipe}\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca73f7cf-5272-4111-993d-a866591b71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "# final_chain = {\"recipe\" : chef_chain} | veg_chain\n",
    "\n",
    "# final_chain.invoke({\n",
    "#     \"cuisine\": \"indian\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf16bcb1-ea97-408a-8f8b-85eae74d5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"question\": \"What do you know about France?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         Here is what I know:\n",
    "#         Capital: Paris\n",
    "#         Language: French\n",
    "#         Food: Wine and Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Italy?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Rome\n",
    "#         Language: Italian\n",
    "#         Food: Pizza and Pasta\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Greece?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Athens\n",
    "#         Language: Greek\n",
    "#         Food: Souvlaki and Feta Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# chat.predict(\"What do you know about France?\")\n",
    "\n",
    "# example_template = \"\"\"\n",
    "#     Human: {question}\n",
    "#     AI: {answer}\n",
    "# \"\"\"\n",
    "\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "#     suffix=\"Human: What do you know about {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "# chain = prompt | chat\n",
    "\n",
    "# chain.invoke({\"country\": \"Germany\"})\n",
    "# chain.invoke({\"country\": \"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "093b3e4a-c2c7-424d-a8d5-ce8f839894e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"country\": \"France\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         Here is what I know:\n",
    "#         Capital: Paris\n",
    "#         Language: French\n",
    "#         Food: Wine and Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"country\": \"Italy\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Rome\n",
    "#         Language: Italian\n",
    "#         Food: Pizza and Pasta\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"country\": \"Greece\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Athens\n",
    "#         Language: Greek\n",
    "#         Food: Souvlaki and Feta Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "#     suffix=\"Human: What do you know about {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# chain = prompt | chat\n",
    "\n",
    "# chain.invoke({\"country\": \"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f089c7e-8935-4afe-a23e-7fd892ddd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "# from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "# examples = [\n",
    "#     {\n",
    "#         \"question\": \"What do you know about France?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         Here is what I know:\n",
    "#         Capital: Paris\n",
    "#         Language: French\n",
    "#         Food: Wine and Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Italy?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Rome\n",
    "#         Language: Italian\n",
    "#         Food: Pizza and Pasta\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What do you know about Greece?\",\n",
    "#         \"answer\": \"\"\"\n",
    "#         I know this:\n",
    "#         Capital: Athens\n",
    "#         Language: Greek\n",
    "#         Food: Souvlaki and Feta Cheese\n",
    "#         Currency: Euro\n",
    "#         \"\"\",\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# class RandomExampleSelector(BaseExampleSelector):\n",
    "#     def __init__(self, examples):\n",
    "#         self.examples = examples\n",
    "\n",
    "#     def add_example(self, example):\n",
    "#         self.examples.append(example)\n",
    "\n",
    "#     def select_examples(self, input_variables):\n",
    "#         from random import choice\n",
    "\n",
    "#         return [choice(self.examples)]\n",
    "        \n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=80\n",
    "# )\n",
    "\n",
    "# example_selector = RandomExampleSelector(\n",
    "#     examples=examples,\n",
    "# )\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     example_selector=example_selector,\n",
    "#     suffix=\"Human: What do you know about {country}?\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "\n",
    "# prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "51792350-b0fe-4a86-899c-08bd9069b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt=load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "# prompt.format(country=\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "203cd41a-1462-48c7-b51d-9263cb34bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "# intro = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     You are a role playing assistant. And you are impersonating a {character}.\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# example = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     This is an example of how you talk:\n",
    "\n",
    "#     Human: {example_question}\n",
    "#     You: {example_answer}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# start = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     {intro}\n",
    "\n",
    "#     {example}\n",
    "\n",
    "#     Human: {question}\n",
    "#     You:\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# final = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     {intro}\n",
    "\n",
    "#     {example}\n",
    "\n",
    "#     {start}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# prompts = [\n",
    "#     (\"intro\", intro),\n",
    "#     (\"example\", example),\n",
    "#     (\"start\", start),\n",
    "# ]\n",
    "\n",
    "# full_prompt = PipelinePromptTemplate(\n",
    "#     final_prompt=final, \n",
    "#     pipeline_prompts=prompts\n",
    "# )\n",
    "\n",
    "# # full_prompt.format(\n",
    "# #     character=\"Pirate\",\n",
    "# #     example_question=\"What is your location?\",\n",
    "# #     example_answer=\"Arrg! That is a secret! Arg arg!!\",\n",
    "# #     question=\"What is ur fav food?\"\n",
    "# # )\n",
    "\n",
    "# chain = full_prompt | chat\n",
    "\n",
    "# chain.invoke({\n",
    "#     \"character\":\"Pirate\",\n",
    "#     \"example_question\":\"What is your location?\",\n",
    "#     \"example_answer\":\"Arrg! That is a secret! Arg arg!!\",\n",
    "#     \"question\":\"What is ur fav food?\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f8d4244-6710-4953-ab79-5f79241514d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_llm_cache, set_debug\n",
    "# from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache()) # cache data\n",
    "# set_llm_cache(SQLiteCache(\"cache.db\")) # set cache and save it to database\n",
    "# set_debug(True) # show logs\n",
    "\n",
    "# chat.predict(\"How do you make Italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa1bbb7d-3433-4684-869f-488b636947dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it answers immediately\n",
    "# chat.predict(\"How do you make Italian pasta?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1df506bd-0dd4-4c7e-880a-2bbe88fb0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# with get_openai_callback() as usage: \n",
    "#     a = chat.predict(\"What is the recipe for soju\")\n",
    "#     b = chat.predict(\"What is the recipe for bread\")\n",
    "#     print(a, b, \"\\n\")\n",
    "#     # print(usage)\n",
    "#     # print(usage.total_cost)\n",
    "#     # print(usage.total_tfrom langchain.llms.loading import load_llm\n",
    "# okens) # completion_tokens 도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dc77c888-fa66-4e1e-8fd3-33f343cf36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory\n",
    "\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory = ConversationBufferMemory() # String 형식으로 출력\n",
    "# memory = ConversationBufferMemory(return_messages=True) # chat model 형식으로 출력\n",
    "\n",
    "# memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "\n",
    "# memory.load_memory_variables({}) # 비효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4d1fbd86-3746-4725-902a-238ad4b74ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최근 n개 메세지만 저장 -> the most recent conversation\n",
    "# from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# memory = ConversationBufferWindowMemory(\n",
    "#     return_messages=True,\n",
    "#     k= 4, # 몇 개를 저장할지 설정\n",
    "# )\n",
    "\n",
    "# def add_message(input, output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# add_message(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9639c85b-06c8-4fcb-86be-c13806d27685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(2,2)\n",
    "# add_message(3,3)\n",
    "# add_message(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3a630f5f-8bff-4fe7-9727-0e663cbaa581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b760ff5-9c52-4d33-80c1-acef09b902ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23c7f2a6-0f80-4bb3-9795-7190b9632b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({}) # (1,1) 이 사라진 것을 확인 가능 -> 이전 대화를 기억할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e320eb3-bb92-4c02-8201-da1bf1dd463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary -> useful for a very long conversation\n",
    "# 글자수가 많기 때문에 초반에는 더 많은 메모리와 토큰을 차지함\n",
    "# 그러나 대화가 진행될수록 저장된 메세지가 매우 많아지면서 모두 연결됨\n",
    "# 요약이 토큰 양을 줄일 수 있음 -> 더 효율적\n",
    "\n",
    "# from langchain.memory import ConversationSummaryMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# def add_message(input,output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# def get_history():\n",
    "#     return memory.load_memory_variables({})\n",
    "\n",
    "# add_message(\"Hi, I'm Camille. I live in South Korea.\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5a49d0e2-257b-488a-8928-c6c1c1cfc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"South Korea is so pretty.\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a33a261-9f00-4891-8f06-a13666e8d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4964a1a1-aa02-4c7c-af97-1612c653162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 최근 메세지, 가장 오래된 메세지 모두 요약\n",
    "# window buffer memory + buffer window memory\n",
    "\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=150, # 최대 토큰 수\n",
    "#     return_messages=True, # chat model\n",
    "# )\n",
    "\n",
    "# def add_message(input,output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# def get_history():\n",
    "#     return memory.load_memory_variables({})\n",
    "\n",
    "# add_message(\"Hi, I'm Camille. I live in South Korea.\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "93194d8f-c903-4db1-8830-f648ab74f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b084735f-c406-4de5-a905-8e35e2fd4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"South Korea is so pretty.\", \"I wish I could go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18c09746-866b-4ce3-a9bc-6961cf649402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "409fbd8e-12f8-4d9f-b242-1b4245e2c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03644869-113f-4ebc-b473-55de7db3b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f6b64b4-1a38-44b0-a101-e9a711d0dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a70b027e-18ee-49dd-9b2c-224bf4762393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2032896b-2649-4cc8-9c92-77334feeb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is Colombia from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5dcb57f1-95bf-488e-ac27-c0e79fef57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d1c508f-521b-4b26-a946-94cce1a25ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"How far is China from Peru?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "258c9003-6e1b-42f1-bb5c-520ee083a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "20fb8852-584d-4adf-a6c7-24b1a7166b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationKGMemory (Knowledge Graph)\n",
    "# 가장 중요한 것들만 뽑아내는 요약본 -> 대화에서 entity 추출\n",
    "\n",
    "# from langchain.memory import ConversationKGMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationKGMemory(\n",
    "#     llm=llm,\n",
    "#     return_messages=True, # chat model\n",
    "# )\n",
    "\n",
    "# def add_message(input,output):\n",
    "#     memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "# add_message(\"Hi, I'm Camille. I live in South Korea.\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71990c5a-3e0a-485e-b3ab-f11f2b564211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({\"input\":\"who is Camille\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "39439124-a281-4a88-bae0-08dcb90ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_message(\"Camille likes Kimchi\", \"Sounds delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f907f8b3-ca49-4bc4-8f2d-08b4adfa897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({\"input\":\"what does Camille like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a23ee3a0-b195-4a11-937c-4895c4b9c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory on LLMChain\n",
    "# interaction의 토큰 수가 limit(80)보다 많으면 가장 오래된 interaction을 요약해줌\n",
    "\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=80,\n",
    "#     # verbose=True, # log\n",
    "# )\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=PromptTemplate.from_template(\"{question}\")\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Camille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "37465a01-d319-477a-891e-d69973c804c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ac7fe55-e0e3-473c-8b35-62a6b68bf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"What's my name\") # it doesn't work -> history isn't added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2bbe02d0-e033-4472-9826-b70208fd29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({}) # Update는 진행하지만 프롬프트에 그 히스토리를 반영하지 않는다 -> template 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "46671d1d-9053-4238-b2fa-7b2c36f27a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 메모리 객체 생성 (memory_key 설정 포함)\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"chat_history\" # 템플릿에서 동일한 이름 사용해야 함\n",
    "# )\n",
    "\n",
    "# template = \"\"\"\n",
    "#     You are a helpful AI talking to a human. \n",
    "\n",
    "#     {chat_history}\n",
    "#     Human:{question}\n",
    "#     You:\n",
    "# \"\"\"\n",
    "\n",
    "# # 템플릿 적용\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=PromptTemplate.from_template(template)\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Camille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "88569369-da69-445f-8046-2f0243ba2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"I live in Seoul\") # prompt가 history를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35fd7753-ef78-4926-82d8-5ead92a4b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"What's my name\") # it remembers my name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bb1d640a-5311-4bcc-8135-4ac552ffb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory.load_memory_variables({}) # 텍스트 형식으로 출력됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "63108b4b-4c73-42ee-8dd7-6d43ab437809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat based memory: 대화 기반의 형식으로 받고 싶다면 \n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"chat_history\",\n",
    "#     return_messages=True\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful AI talking to human\"),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"), # 우리는 메세지가 얼마나 많고 누구로부터 왔는지 모름. 위의 memory class로 대체될 것.\n",
    "#     (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=prompt,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Camille\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "16b13f0c-d2c2-4997-b952-3dbf82c5ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "503aae30-7b4b-4ebd-b276-7f51e1b0aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.predict(question=\"What's my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e4ea2c7-82ae-4d22-8ae6-2cb4ff1ae3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 커스터마이징 메모리: LCEL based Memory\n",
    "\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema.runnable import RunnablePassthrough # Prompt가 format 되기 전에 함수 실행시키는 걸 허락해준다\n",
    "# from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# # memory와 prompt는 변하지 않는다.\n",
    "# memory = ConversationSummaryBufferMemory(\n",
    "#     llm=llm,\n",
    "#     max_token_limit=120,\n",
    "#     memory_key=\"history\",\n",
    "#     return_messages=True\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful AI talking to human\"),\n",
    "#     MessagesPlaceholder(variable_name=\"history\"), # 우리는 메세지가 얼마나 많고 누구로부터 왔는지 모름. 위의 memory class로 대체될 것.\n",
    "#     (\"human\", \"{question}\")\n",
    "# ]) \n",
    "\n",
    "# def load_memory(_): # _: ignore\n",
    "#     # print(input)\n",
    "#     return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "# # 체인을 호출하는 함수 생성 -> 수동으로 메모리 관리하는 것보다 편리한 방법\n",
    "# def invoke_chain(question): \n",
    "#     result = chain.invoke({\"question\": question})\n",
    "#     memory.save_context(\n",
    "#         {\"input\":question}, \n",
    "#         {\"output\":result.content}\n",
    "#     )\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "82fb7199-b05f-4b51-b0a7-ce75930aab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke_chain(\"My name is Camille\") #  대화 내용이 메모리에 저장되고, LLM은 이전 대화 내용을 기억하면서 응답함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d81f752e-077d-4334-b3b8-6d9549a8ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke_chain(\"what is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b8eaced5-d9fc-49aa-a0dd-af0c30c3cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "52d87956-3719-4128-b2b9-e80ac5b774d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "# loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9a782d7f-a51e-4aa7-993d-09a0686f7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# # loader.load()\n",
    "# len(loader.load()) # 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cdfe127d-5949-48ec-a58d-90dcdd1671f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50\n",
    "# )\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# splitter.split_documents(docs)\n",
    "\n",
    "# len(loader.load_and_split(text_splitter=splitter)) # 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ecc21628-c475-4241-9e8f-50d9921c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\",\n",
    "#     chunk_size=600,\n",
    "#     chunk_overlap=100\n",
    "# )\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "# splitter.split_documents(docs)\n",
    "\n",
    "# len(loader.load_and_split(text_splitter=splitter)) # 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1afd438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     separator=\"\\n\",\n",
    "#     chunk_size=600,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len, # counting texts (different from human's counting method )\n",
    "# )\n",
    "# # tokens != characters\n",
    "# # tiktoken은 tokens 수를 세는 것 -> from_tiktoken_encoder을 사용함으로써 사람과 동일한 방법으로 텍스트 카운팅이 가능\n",
    "\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b08ca005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vector store\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embedder= OpenAIEmbeddings()\n",
    "\n",
    "# # vector = embedder.embed_query(\"Hi\")\n",
    "# # len(vector) # how many dimensions? 1536\n",
    "\n",
    "# vector = embedder.embed_documents([\n",
    "#   \"hi\",\n",
    "#   \"how\",\n",
    "#   \"are\",\n",
    "#   \"you + longer sentenes\"\n",
    "# ])\n",
    "\n",
    "# # vector\n",
    "# # len(vector) # 4\n",
    "\n",
    "# print(len(vector), len(vector[0])) # 4개 벡터, 1536개의 차원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c4628142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.storage import LocalFileStore\n",
    "\n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "\n",
    "# vectorstore = Chroma.from_documents(docs, cached_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ff730dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = vectorstore.similarity_search(\"where does winston live?\")\n",
    "\n",
    "# len(results) # 4\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3b76b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "\n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "\n",
    "# vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#   llm=llm,\n",
    "#   chain_type=\"map_rerank\", # map_reduce, stuff, refine, map_rerank\n",
    "#   retriever=vectorstore.as_retriever(),\n",
    "# )\n",
    "\n",
    "# chain.run(\"where does winston live?\") # 1st floor 'Winston Smith lives in Victory Mansions.'\n",
    "# chain.run(\"Describe Victory Mansions\") # 2nd floor 'Victory Mansions is a place where Winston lives. It is a place that is not very nice.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ca2ff56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#   temperature=0.1,\n",
    "# )\n",
    "\n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "\n",
    "# vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# retriever = vectorstore.as_retriever() \n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#   (\"system\", \"You are a helpful assistant. Answer the questions using only the following context. If you don't know the answer, just say you don't know, don't make it up:\\n{context}\"),\n",
    "#   (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# chain = (\n",
    "#   {\n",
    "#     \"context\": retriever, \n",
    "#     \"question\": RunnablePassthrough(), \n",
    "#     # \"extra\": RunnablePassthrough(),\n",
    "#   } \n",
    "#   | prompt \n",
    "#   | llm\n",
    "# )\n",
    "\n",
    "# chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9178240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import UnstructuredFileLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.storage import LocalFileStore\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# \n",
    "# llm = ChatOpenAI(\n",
    "#   temperature=0.1,\n",
    "# )\n",
    "# \n",
    "# cache_dir = LocalFileStore(\"./.cache/\")\n",
    "# \n",
    "# splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "#   separator=\"\\n\",\n",
    "#   chunk_size=600,\n",
    "#   chunk_overlap=100,\n",
    "# )\n",
    "# loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "# \n",
    "# docs = loader.load_and_split(text_splitter=splitter)\n",
    "# \n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# \n",
    "# cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "#   embeddings, cache_dir\n",
    "# ) \n",
    "# \n",
    "# vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "# \n",
    "# retriever = vectorstore.as_retriever() \n",
    "# \n",
    "# # list of docs\n",
    "# \n",
    "# # for doc in list of docs | prompt | llm \n",
    "# \n",
    "# # for response in list of llms response | put them all together \n",
    "# \n",
    "# # final doc | prompt | llm\n",
    "# \n",
    "# map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "#   [\n",
    "#     (\n",
    "#       \"system\",\n",
    "#       \"\"\"\n",
    "#       Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "#       Return any relevant text verbatim.\n",
    "#       -----\n",
    "#       {context}\n",
    "#       \"\"\",\n",
    "#     ),\n",
    "#     (\"human\", \"{question}\"),\n",
    "#   ]\n",
    "# )\n",
    "# \n",
    "# map_doc_chain =  map_doc_prompt | llm \n",
    "# \n",
    "# def map_docs(inputs):\n",
    "#   # print(inputs)\n",
    "#   documents = inputs[\"documents\"]\n",
    "#   question = inputs[\"question\"]\n",
    "#   # results = []\n",
    "#   # for document in documents:\n",
    "#   #   result = map_doc_chain.invoke({\n",
    "#   #     \"context\": document.page_content,\n",
    "#   #     \"question\": question,\n",
    "#   #   }).content\n",
    "#   #   results.append(result)\n",
    "#   # results = \"\\n\\n\".join(results)\n",
    "#   # return results\n",
    "# \n",
    "#   return \"\\n\\n\".join(map_doc_chain.invoke({\n",
    "#     \"context\": doc.page_content,\n",
    "#     \"question\": question,\n",
    "#     }).content\n",
    "#     for doc in documents\n",
    "#   ) # list of docs\n",
    "# \n",
    "# \n",
    "# map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    "# RunnableLambda(map_docs)\n",
    "# \n",
    "# # {\n",
    "# #   \"documents\": [Documents],\n",
    "# #   \"question\": \"Describe Victory Mansions\"\n",
    "# # }\n",
    "# \n",
    "# final_prompt = ChatPromptTemplate.from_messages([\n",
    "#   (\n",
    "#     \"system\", \n",
    "#     \"\"\"\n",
    "#     Given the following extracted parts of a long document and a question, \n",
    "#     create a final answer. \n",
    "#     If you don't know the answer, just say you don't know, don't make it up.\n",
    "#     -----\n",
    "#     {context}\n",
    "#     \"\"\",\n",
    "#   ),\n",
    "#   (\"human\", \"{question}\"),\n",
    "# ])\n",
    "# \n",
    "# chain = {\"context\": map_chain,\"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "# \n",
    "# chain.invoke(\"Where does Winston go to work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1104dba4-8e84-42a9-af29-c598477c9903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fe62e0b781d0337b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"[INST]What is the meaning of {word}[/INST]\")\n",
    "\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     model_kwargs={\n",
    "#         \"max_new_tokens\": 250,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# chain = prompt | llm\n",
    "\n",
    "# chain.invoke({\"word\": \"potato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3c03daf5-9d43-4d4a-9358-208c1cf1b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms.gpt4all import GPT4All\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# \n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"You are a helpful assistant that defines words. Define this word: {word}.\"\n",
    "# )\n",
    "# \n",
    "# llm = GPT4All(\n",
    "#     model=\"./falcon.bin\",\n",
    "# )\n",
    "# \n",
    "# chain = prompt | llm\n",
    "# \n",
    "# chain.invoke({\"word\": \"tomato\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c10f8facdd057bcd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# function = {\n",
    "#     \"name\": \"create_quiz\",\n",
    "#     \"description\": \"function that takes a list of questions and answers and returns a quiz\",\n",
    "#     \"parameters\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"questions\": {\n",
    "#                 \"type\": \"array\",\n",
    "#                 \"items\": {\n",
    "#                     \"type\": \"object\",\n",
    "#                     \"properties\": {\n",
    "#                         \"question\": {\n",
    "#                             \"type\": \"string\",\n",
    "#                         },\n",
    "#                         \"answers\": {\n",
    "#                             \"type\": \"array\",\n",
    "#                             \"items\": {\n",
    "#                                 \"type\": \"object\",\n",
    "#                                 \"properties\": {\n",
    "#                                     \"answer\": {\n",
    "#                                         \"type\": \"string\",\n",
    "#                                     },\n",
    "#                                     \"correct\": {\n",
    "#                                         \"type\": \"boolean\",\n",
    "#                                     },\n",
    "#                                 },\n",
    "#                                 \"required\": [\"answer\", \"correct\"],\n",
    "#                             },\n",
    "#                         },\n",
    "#                     },\n",
    "#                     \"required\": [\"question\", \"answers\"],\n",
    "#                 },\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"questions\"],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     temperature=0.1,\n",
    "# ).bind(\n",
    "#     function_call={\n",
    "#         \"name\": \"create_quiz\",\n",
    "#     },\n",
    "#     functions=[\n",
    "#         function,\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\"Make a quiz about {city}\")\n",
    "\n",
    "# chain = prompt | llm\n",
    "\n",
    "# response = chain.invoke({\"city\": \"rome\"})\n",
    "\n",
    "\n",
    "# response = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "\n",
    "# response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6039749a-15cd-41ff-a867-bac6b9e4f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# for question in json.loads(response)[\"questions\"]:\n",
    "#     print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ea7637",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PERSONA_AI\\AppData\\Local\\Temp\\ipykernel_24000\\1430727231.py:63: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 2354.64, 'b': 9245.4}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m11600.039999999999\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 11600.04, 'b': 66.2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m11666.240000000002\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 11666.24, 'b': 0.99}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m11667.23\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `CalculatorTool` with `{'a': 11667.23, 'b': 5325.3029}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m16992.5329\u001b[0m\u001b[32;1m\u001b[1;3mThe total cost of $2354.64 + $9245.4 + $66.2 + $0.99 + $5325.3029 is $16,992.53.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Cost of $2354.64 + $9245.4 + $66.2 + $0.99 + $5325.3029',\n",
       " 'output': 'The total cost of $2354.64 + $9245.4 + $66.2 + $0.99 + $5325.3029 is $16,992.53.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Type\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import StructuredTool, Tool, BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "\n",
    "# # Use StructuredTool\n",
    "# def plus(a, b):\n",
    "#     return a + b\n",
    "\n",
    "# agent = initialize_agent(\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "#     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     tools=[\n",
    "#         StructuredTool.from_function(\n",
    "#             func=plus,\n",
    "#             name=\"Sum Calculator\",\n",
    "#             description=\"Use this to perform sums of two numbers. This tool takes two arguments, both should be numbers.\",\n",
    "#         ),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Use Zero-Shot-React\n",
    "# def plus(inputs):\n",
    "#     a, b = inputs.split(\",\")\n",
    "#     return float(a) + float(b)\n",
    "\n",
    "# agent = initialize_agent(\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "#     handle_parsing_errors = True,\n",
    "#     tools=[\n",
    "#         Tool.from_function(\n",
    "#             func=plus,\n",
    "#             name=\"Sum Calculator\",\n",
    "#             description=\"Use this to perform sums of two numbers. Use this tool by sending a pair of number separated by a comma.\\nExample: 1,2\",\n",
    "#         ),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# # Use BaseTool\n",
    "class CalculatorToolArgsSchema(BaseModel):\n",
    "    a: float = Field(description=\"The first number\")\n",
    "    b: float = Field(description=\"The second number\")\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name: str = \"CalculatorTool\"\n",
    "    description: str = \"\"\"\n",
    "    Use this to perform sums of two numbers.\n",
    "    The first and second arguments should be numbers.\n",
    "    Only receives two arguments.\n",
    "    \"\"\"\n",
    "    args_schema: Type[BaseModel] = CalculatorToolArgsSchema\n",
    "\n",
    "    def _run(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors = True,\n",
    "    tools=[ CalculatorTool() ],\n",
    ")\n",
    "\n",
    "prompt = \"Cost of $2354.64 + $9245.4 + $66.2 + $0.99 + $5325.3029\"\n",
    "\n",
    "# llm.invoke(prompt) # LLM은 자체적으로 정확한 계산을 할 수 없음 -> agent를 사용\n",
    "\n",
    "agent.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ffe63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
