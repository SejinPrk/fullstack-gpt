from langchain.callbacks.base import BaseCallbackHandler
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.schema.messages import AIMessage, HumanMessage
import streamlit as st

# ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history"
    )


# Streamlit í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)

# LLM ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤
class ChatCallbackHandler(BaseCallbackHandler):
    message = "" # ë¹ˆ ë©”ì„¸ì§€ë¡œ ì‹œì‘

    # selfëŠ” class ì „ì²´ë¥¼ ì°¸ì¡°í•˜ë¯€ë¡œ ì‚¬ìš© (ê°ì²´ì§€í–¥ ë¬¸ë²•)
    # LLMì´ ìƒˆ í† í° ìƒì„± -> empty box ìƒì„±ë¨
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty() # ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±

    # LLMì´ ìƒì„± ì™„ë£Œ -> ìµœì¢… ë©”ì‹œì§€ ì €ì¥
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai") # ë©”ì‹œì§€ ìƒì„±ì´ ì™„ë£Œë˜ë©´ ì„¸ì…˜ì— ì €ì¥

    # LLMì´ ìƒˆ í† í°ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ í˜¸ì¶œ -> ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token # í† í°ì„ ë©”ì„¸ì§€ì— ì¶”ê°€í•˜ëŠ” ë°©ì‹
        self.message_box.markdown(self.message) # UI ì—…ë°ì´íŠ¸

# OpenAI ëª¨ë¸ ì„¤ì • - ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” ë° ì½œë°± í•¸ë“¤ëŸ¬ ì—°ê²°
llm = ChatOpenAI(
    temperature=0.1, # ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ ì‘ë‹µ ìƒì„±
    streaming=True, # í† í° ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    callbacks = [
        ChatCallbackHandler(), # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬ ì—°ê²°
    ]
)

# íŒŒì¼ ì„ë² ë”© í•¨ìˆ˜ - ìºì‹± ì ìš©ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” (íŒŒì¼ì´ ë‹¬ë¼ì§ˆ ê²½ìš°ì—ë§Œ ì‹¤í–‰ë¨)
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    # íŒŒì¼ ì½˜í…ì¸  ì½ê¸°
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"

    # íŒŒì¼ ë¡œì»¬ì— ì €ì¥
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ì„ë² ë”© ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì • - ì²­í¬ í¬ê¸°ì™€ ì˜¤ë²„ë© ì¡°ì •
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600, # ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸°
        chunk_overlap=100, # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•œ ì²­í¬ ê°„ ê²¹ì¹¨ ì •ë„
    )

    # íŒŒì¼ ë¡œë“œ ë° ë¶„í• 
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)

    # OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë° ìºì‹±
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ê²€ìƒ‰ê¸° ë°˜í™˜
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

# ëŒ€í™” ê¸°ë¡ì„ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
def format_chat_history(chat_history):
    formatted_history = ""
    for message in chat_history:
        if isinstance(message, HumanMessage):
            formatted_history += f"Human: {message.content}\n"
        elif isinstance(message, AIMessage):
            formatted_history += f"AI: {message.content}\n"
    return formatted_history

# ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

    # ë©”ëª¨ë¦¬ì—ë„ ë©”ì‹œì§€ ì¶”ê°€
    if role == "human":
        st.session_state["memory"].chat_memory.add_message(HumanMessage(content=message))
    elif role == "ai":
        st.session_state["memory"].chat_memory.add_message(AIMessage(content=message))

# ë©”ì‹œì§€ë¥¼ UIì— í‘œì‹œí•˜ê³  í•„ìš”ì‹œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role): # Streamlit ì±„íŒ… ë©”ì‹œì§€ UI ì»´í¬ë„ŒíŠ¸
        st.markdown(message)
    if save:
        save_message(message, role)

# ì±„íŒ… ê¸°ë¡ì„ UIì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False, # ì´ë¯¸ ì €ì¥ëœ ë©”ì‹œì§€ì´ë¯€ë¡œ ë‹¤ì‹œ ì €ì¥í•˜ì§€ ì•ŠìŒ
        )

# ë¬¸ì„œì—ì„œ ê°œí–‰ ë¬¸ìë¡œ êµ¬ë¶„ëœ ê²ƒë“¤ì„ í•˜ë‚˜ì˜ stringìœ¼ë¡œ í•©ì¹œë‹¤.
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

# LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context and chat history.
            If you don't know the answer just say you don't know. Don't make anything up.

            Context: {context}

            Chat History:
            {chat_history}
            """
        ),
        ("human", "{question}")
    ]
)

# ì•± ì œëª© ì„¤ì •
st.title("DocumentGPT")

# ì•± ì†Œê°œ ë©”ì‹œì§€
st.markdown(
    """
    Welcome!

    Use this chatbot to ask questions to an AI about your files!
    
    Upload your files on the sidebar.
    """
)

# ì‚¬ì´ë“œë°”ì— íŒŒì¼ ì—…ë¡œë” ë°°ì¹˜
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"], # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ ì§€ì •
    )

# íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°
if file:
    # íŒŒì¼ ì„ë² ë”© ë° ê²€ìƒ‰ê¸° ì¤€ë¹„
    retriever = embed_file(file)

    # ì‹œì‘ ë©”ì‹œì§€ í‘œì‹œ (ì €ì¥x)
    send_message("I'm ready! Ask away!", "ai", save=False)

    # ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    paint_history()

    # ì‚¬ìš©ì ì…ë ¥ í•„ë“œ í‘œì‹œ
    message = st.chat_input("Ask anything about your file...")

    # ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•œ ê²½ìš°
    if message:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        send_message(message, "human")

        # RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„±:
        # 1. ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ ì¤€ë¹„
        # 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        # 3. LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        chain = (
                {
                    "context": retriever | RunnableLambda(format_docs),  # ë¬¸ì„œ ê²€ìƒ‰ ë° í¬ë§·íŒ…
                    "question": RunnablePassthrough(),  # ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì „ë‹¬
                    "chat_history": RunnableLambda(
                        lambda _: format_chat_history(
                            st.session_state.get("memory", ConversationBufferMemory(
                                return_messages=True,
                                memory_key="chat_history"
                            )).chat_memory.messages
                        )
                    )
                }
                | prompt  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
                | llm  # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        )

        # AI ì‘ë‹µ ë©”ì‹œì§€ UI ì»´í¬ë„ŒíŠ¸ ìƒì„± ë° ì²´ì¸ ì‹¤í–‰
        with st.chat_message("ai"):
            chain.invoke(message) # ë©”ì‹œì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì²´ì¸ ì‹¤í–‰
else:
    # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state["messages"] = []
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history"
    )
